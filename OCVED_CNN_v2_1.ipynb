{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OCVED_CNN_v2.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlejandroBeltranA/OCVED-ML/blob/master/OCVED_CNN_v2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XywVevXGELwP",
        "colab_type": "text"
      },
      "source": [
        "## OCVED Neural Network\n",
        "\n",
        "This is the second script of four used in Osorio & Beltran (2020)\n",
        "\n",
        "Here I build a series of convolutional neural networks to classify the training data. The CNN's perform well but not as great as the logistic regression. CNN's require much more data for accurate predictions. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05KHeBN4EVrc",
        "colab_type": "text"
      },
      "source": [
        "First step is to load our google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_La8LEaAqKo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "fb3abcbe-4626-4bb5-a52a-e3befa3530bb"
      },
      "source": [
        "# Upload the train file from your local drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMrLWUQOEYXw",
        "colab_type": "text"
      },
      "source": [
        "Here we are using the preprocessed text I already cleaned out in a previous step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxnWTmyLAuHP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "15969a7c-d86d-4d4c-9404-22b4a6009265"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/Data/OCVED/Classifier/universe/preprocessed_text.csv')\n",
        "df['source'] = 'ocved'\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>index</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>file_id</th>\n",
              "      <th>label_1</th>\n",
              "      <th>category</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>margaritas chis ntx elementos ejercito exicano...</td>\n",
              "      <td>Accept</td>\n",
              "      <td>20000105001_NAC.txt</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>ocved</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>ntx policia federal preventiva pfp informo ult...</td>\n",
              "      <td>Accept</td>\n",
              "      <td>20000105002_NAC.txt</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>ocved</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>ntx elementos policia judicial federal pjf ase...</td>\n",
              "      <td>Accept</td>\n",
              "      <td>20000106001_NAC.txt</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>ocved</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>monterrey ntx policia ministerial reporto homb...</td>\n",
              "      <td>Accept</td>\n",
              "      <td>20000106002_NAC.txt</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>ocved</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>ntx elementos policia judicial federal pjf ase...</td>\n",
              "      <td>Accept</td>\n",
              "      <td>20000106003_NAC.txt</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>ocved</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60816</th>\n",
              "      <td>60816</td>\n",
              "      <td>60832</td>\n",
              "      <td>carril lateral autopista queretaro quedar cuer...</td>\n",
              "      <td>Reject</td>\n",
              "      <td>20181231__639379415.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>ocved</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60817</th>\n",
              "      <td>60817</td>\n",
              "      <td>60833</td>\n",
              "      <td>fuga agua romper pavimento calle lluvia coloni...</td>\n",
              "      <td>Reject</td>\n",
              "      <td>20181231__639382582.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>ocved</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60818</th>\n",
              "      <td>60818</td>\n",
              "      <td>60834</td>\n",
              "      <td>nuevo casas grandes hombre vida localizar hace...</td>\n",
              "      <td>Reject</td>\n",
              "      <td>20181231__639386332.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>ocved</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60819</th>\n",
              "      <td>60819</td>\n",
              "      <td>60835</td>\n",
              "      <td>nuevo casas grandes ciudad total cuatro person...</td>\n",
              "      <td>Reject</td>\n",
              "      <td>20181231__639386333.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>ocved</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60820</th>\n",
              "      <td>60820</td>\n",
              "      <td>60836</td>\n",
              "      <td>quintana roo violencia azotar entidad llego pa...</td>\n",
              "      <td>Accept</td>\n",
              "      <td>20181231__639388959.txt</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>ocved</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60821 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  index  ... category source\n",
              "0               0      0  ...        1  ocved\n",
              "1               1      1  ...        1  ocved\n",
              "2               2      2  ...        1  ocved\n",
              "3               3      3  ...        1  ocved\n",
              "4               4      4  ...        1  ocved\n",
              "...           ...    ...  ...      ...    ...\n",
              "60816       60816  60832  ...        0  ocved\n",
              "60817       60817  60833  ...        0  ocved\n",
              "60818       60818  60834  ...        0  ocved\n",
              "60819       60819  60835  ...        0  ocved\n",
              "60820       60820  60836  ...        1  ocved\n",
              "\n",
              "[60821 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeQDQNBkEeRQ",
        "colab_type": "text"
      },
      "source": [
        "Here I pull the GSR classifications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1PDoisBxRL0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "17c21e86-8df2-4ca5-e146-500675603566"
      },
      "source": [
        "gsd = pd.read_csv('/content/drive/My Drive/Data/OCVED/Classifier/predictions_v2/correct_classification.csv')\n",
        "gsd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_id</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20000105001_NAC.txt</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20000105002_NAC.txt</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20000106001_NAC.txt</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20000106002_NAC.txt</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20000106003_NAC.txt</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60801</th>\n",
              "      <td>20181231__639379415.txt</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60802</th>\n",
              "      <td>20181231__639382582.txt</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60803</th>\n",
              "      <td>20181231__639386332.txt</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60804</th>\n",
              "      <td>20181231__639386333.txt</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60805</th>\n",
              "      <td>20181231__639388959.txt</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60806 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                       file_id  correct\n",
              "0          20000105001_NAC.txt        1\n",
              "1          20000105002_NAC.txt        1\n",
              "2          20000106001_NAC.txt        1\n",
              "3          20000106002_NAC.txt        1\n",
              "4          20000106003_NAC.txt        1\n",
              "...                        ...      ...\n",
              "60801  20181231__639379415.txt        0\n",
              "60802  20181231__639382582.txt        0\n",
              "60803  20181231__639386332.txt        0\n",
              "60804  20181231__639386333.txt        0\n",
              "60805  20181231__639388959.txt        1\n",
              "\n",
              "[60806 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PjiSLkuxWZz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "0527c7a7-494e-42d4-a862-7102a3c0ca0e"
      },
      "source": [
        "df = pd.merge(df, gsd, on = \"file_id\")\n",
        "df = df[[\"file_id\", \"text\", \"source\", \"correct\"]]\n",
        "df = df.rename(columns = {\"correct\":\"label\"})\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_id</th>\n",
              "      <th>text</th>\n",
              "      <th>source</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20000105001_NAC.txt</td>\n",
              "      <td>margaritas chis ntx elementos ejercito exicano...</td>\n",
              "      <td>ocved</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20000105002_NAC.txt</td>\n",
              "      <td>ntx policia federal preventiva pfp informo ult...</td>\n",
              "      <td>ocved</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20000106001_NAC.txt</td>\n",
              "      <td>ntx elementos policia judicial federal pjf ase...</td>\n",
              "      <td>ocved</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20000106002_NAC.txt</td>\n",
              "      <td>monterrey ntx policia ministerial reporto homb...</td>\n",
              "      <td>ocved</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20000106003_NAC.txt</td>\n",
              "      <td>ntx elementos policia judicial federal pjf ase...</td>\n",
              "      <td>ocved</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60801</th>\n",
              "      <td>20181231__639379415.txt</td>\n",
              "      <td>carril lateral autopista queretaro quedar cuer...</td>\n",
              "      <td>ocved</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60802</th>\n",
              "      <td>20181231__639382582.txt</td>\n",
              "      <td>fuga agua romper pavimento calle lluvia coloni...</td>\n",
              "      <td>ocved</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60803</th>\n",
              "      <td>20181231__639386332.txt</td>\n",
              "      <td>nuevo casas grandes hombre vida localizar hace...</td>\n",
              "      <td>ocved</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60804</th>\n",
              "      <td>20181231__639386333.txt</td>\n",
              "      <td>nuevo casas grandes ciudad total cuatro person...</td>\n",
              "      <td>ocved</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60805</th>\n",
              "      <td>20181231__639388959.txt</td>\n",
              "      <td>quintana roo violencia azotar entidad llego pa...</td>\n",
              "      <td>ocved</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60806 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                       file_id  ... label\n",
              "0          20000105001_NAC.txt  ...     1\n",
              "1          20000105002_NAC.txt  ...     1\n",
              "2          20000106001_NAC.txt  ...     1\n",
              "3          20000106002_NAC.txt  ...     1\n",
              "4          20000106003_NAC.txt  ...     1\n",
              "...                        ...  ...   ...\n",
              "60801  20181231__639379415.txt  ...     0\n",
              "60802  20181231__639382582.txt  ...     0\n",
              "60803  20181231__639386332.txt  ...     0\n",
              "60804  20181231__639386333.txt  ...     0\n",
              "60805  20181231__639388959.txt  ...     1\n",
              "\n",
              "[60806 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2oqVMctEqfy",
        "colab_type": "text"
      },
      "source": [
        "Before getting into the NN, I test the data on a simple logistic regression. \n",
        "\n",
        "The command below creates the training and testing data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cYmZtk3A7zL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.25, random_state=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvWYaRm-Ezys",
        "colab_type": "text"
      },
      "source": [
        "I'm using count vectorizer here rather than tfidf. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm8dh4pNBG49",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c9e6c7b6-6e09-4248-b4c0-b462e5821780"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(sentences_train)\n",
        "\n",
        "X_train = vectorizer.transform(sentences_train)\n",
        "X_test  = vectorizer.transform(sentences_test)\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<45604x85168 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 3940041 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF75VzqCE3yz",
        "colab_type": "text"
      },
      "source": [
        "Let's run the LR and see how well it performs, this is our baseline comparison. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYCILVPyBQNv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4566e26a-5134-4c39-b91e-27f2599584fd"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression(solver='liblinear')\n",
        "classifier.fit(X_train, y_train)\n",
        "score = classifier.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy:\", score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9358637021444547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQdGz0bQN--O",
        "colab_type": "text"
      },
      "source": [
        "Does pretty ok, let's make things more complicated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXiz2NNgCGdY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "e4933580-8845-450a-a2d0-013c8a4e0b97"
      },
      "source": [
        "!pip install keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGfAd0DPPliL",
        "colab_type": "text"
      },
      "source": [
        "I'm using a simple tokenizer with padding to reduce the size of articles. If in the first 500 words we can't tell if the article is relevant or not then neither will the model. Plus the CNN takes a long time to run, this helps make it a manageable model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-EVeWilERO_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c6880499-929c-43a0-e77b-bcfd1d97d82f"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "\n",
        "print(sentences_train[2])\n",
        "print(X_train[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ntx elementos policia judicial federal pjf asegurar tonelada kilogramo mariguana operativo revision realizado nayarit informo hoy procuraduria general republica pgr dependencia agrego comunicado ejercer supervision vigilancia carretera numero tramo tepic capomal revisar vehiculo marcar kinworth tipo tractor color verde servicio publico federal placa circulacion detallo vehiculo modelo conducir javier valadez sepulveda procedente tonala jalisco dirigir ciudad tijuana baja embargo detener revision integrante pjf delegacion estatal explico dentro automotor encontrar caja madera paquete confeccionados cinta adhesivo color beige interior contenian mariguana peso bruto total kilo gramos pgr dar conocer droga vehiculo detenido poner disposicion agente ministerio publico federacion iniciar averiguacion correspondiente delito salud ntx ago mer mmm\n",
            "[527, 524, 315, 1986, 12, 35, 1267, 1310, 315, 12, 35, 1267, 1310, 257, 1, 1509, 12, 63, 230, 878, 39, 121, 688, 31, 161, 1744, 10, 1065, 1310, 144, 1567, 1222, 102, 48, 5, 159, 3, 2461, 32, 28, 347, 367, 517, 836, 488, 2473, 165, 1772, 1545, 236, 224, 1284, 81, 688, 111, 142, 101, 79, 584, 35, 125, 15, 4963, 1836, 1031, 278, 313, 960, 1468, 1313, 81, 195, 458, 1039, 81, 688, 1799, 836, 16, 386, 121, 129, 1031, 1190, 225, 458, 4385, 22, 12, 632, 504, 4, 1424, 44, 78, 76, 61, 31, 39, 121, 2385, 161, 87, 836, 177, 643, 517, 368, 863, 89, 158, 2218, 1761, 579, 4123, 2461, 105, 70, 4486, 1609, 91, 3496, 78, 504, 187, 826, 903, 39, 121, 2189]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btUzlZVqEZe5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "ece9378a-bf63-4ab9-ca85-99223abb7ea6"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 300\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "print(X_train[0, :])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   8  212  428 1155   88    8    2 1075  126   25  297   14    1   19\n",
            "   28 1361  160  138  316  197  386  552   99  428 1089  969  372   22\n",
            "    8    2   67  118   20  184 1003  241  223  101 1757   28  109 1341\n",
            "   29  322   26  390  316  248   67  351  223  994 1194 2410   49  656\n",
            "  307  390   58  265   29    6 1409  431  292 3859  321  667    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5OcAki2P7f6",
        "colab_type": "text"
      },
      "source": [
        "Here I define the model and internals of the CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl_WBTt-J1uz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
        "    model = Sequential()\n",
        "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(10, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udmWucZSJ3p1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_grid = dict(num_filters=[32, 64, 128],\n",
        "                  kernel_size=[3, 5, 7],\n",
        "                  vocab_size=[5000], \n",
        "                  embedding_dim=[50],\n",
        "                  maxlen=[500])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRS470V4Qlmp",
        "colab_type": "text"
      },
      "source": [
        "Now to the actual model. I am using a randomized grid search, which runs multiple versions of a cnn with different dimensions to see which one does the best job. CNN's require a lot of fine tuning , but given this is not a very big sample size I rely on the random search for testing multiple paramters. It will run 5 different CNN's "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMCOVL7JxEhC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "2f7fe0ee-f284-433c-9b97-f5b5a53cb125"
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "# Main settings\n",
        "epochs = 20\n",
        "embedding_dim = 50\n",
        "maxlen = 300\n",
        "output_file = '/content/drive/My Drive/Data/OCVED/Classifier/universe/output_6.txt'\n",
        "saved_scores = pd.DataFrame()\n",
        "# Run grid search for each source \n",
        "for source, frame in df.groupby('source'):\n",
        "    print('Running grid search for data set :', source)\n",
        "    sentences = df['text'].values\n",
        "    y = df['label'].values\n",
        "\n",
        "    # Train-test split\n",
        "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "        sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "    # Tokenize words\n",
        "    tokenizer = Tokenizer(num_words=5000)\n",
        "    tokenizer.fit_on_texts(sentences_train)\n",
        "    X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "    X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "    # Adding 1 because of reserved 0 index\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "    # Pad sequences with zeros\n",
        "    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "    print (\"preprocessing done\")\n",
        "    # Parameter grid for grid search\n",
        "    param_grid = dict(num_filters=[32, 64, 128],\n",
        "                      kernel_size=[3, 5, 7],\n",
        "                      vocab_size=[vocab_size],\n",
        "                      embedding_dim=[embedding_dim],\n",
        "                      maxlen=[maxlen])\n",
        "    model = KerasClassifier(build_fn=create_model,\n",
        "                            epochs=epochs, batch_size=10,\n",
        "                            verbose=False)\n",
        "    grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
        "                              cv=4, verbose=1, n_iter=5)\n",
        "    print (\"model ready\")\n",
        "    grid_result = grid.fit(X_train, y_train)\n",
        "    saved_scores['NN'] = grid_result\n",
        "\n",
        "    # Evaluate testing set\n",
        "    test_accuracy = grid.score(X_test, y_test)\n",
        "    print (\"CNN Done\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running grid search for data set : ocved\n",
            "preprocessing done\n",
            "model ready\n",
            "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF1134eKLSW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # Save and evaluate results\n",
        "    with open(output_file, 'a') as f:\n",
        "        s = ('Running {} data set\\nBest Accuracy : '\n",
        "             '{:.4f}\\n{}\\nTest Accuracy : {:.4f}\\n\\n')\n",
        "        output_string = s.format(\n",
        "            source,\n",
        "            grid_result.best_score_,\n",
        "            grid_result.best_params_,\n",
        "            test_accuracy)\n",
        "        print(output_string)\n",
        "        f.write(output_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7OEOO6ov4UX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2RMoVuVQ71b",
        "colab_type": "text"
      },
      "source": [
        "After it has finished running I print out the dimensions and reslts for each model and store it in a csv. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wiVkBlNd2f-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"done\")\n",
        "grid_result.cv_results_ \n",
        "save = pd.DataFrame(grid_result.cv_results_)\n",
        "save\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hbfDjJ0voE3",
        "colab_type": "text"
      },
      "source": [
        "Looks like all the CNN's averaged around .92\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4NCYFLZb5yN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save.to_csv(\"/content/drive/My Drive/Data/OCVED/Classifier/results/scores/CNN_scores_v2.1.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}