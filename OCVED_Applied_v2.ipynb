{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OCVED_Applied_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlejandroBeltranA/OCVED-ML/blob/master/OCVED_Applied_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwfmWqSkSzKq",
        "colab_type": "text"
      },
      "source": [
        "# Classifying remaining articles\n",
        "\n",
        "This is the 4th of 4 scripts used in ocved.mx\n",
        "\n",
        "This script uses the LR model trained in the first script to classify the universe of articles collected from EMIS. A total of 188,492 are classified using the model from OCVED_GSR_Trained.v2.4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_MGiDOOQltY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "46eb4b6e-d64f-4e53-a16d-cd3596ffd29c"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrXfam1qTe_l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "7a798421-8170-4c13-ae84-3ecf53266290"
      },
      "source": [
        "# Install tqdm\n",
        "%cd /content/drive/\n",
        "!ls\n",
        "!pip install tqdm\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n",
            "'My Drive'\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o41mzJNSTrWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Packages used\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "#from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, naive_bayes, svm, linear_model\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_score, recall_score, f1_score"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk3CtmUPUBfw",
        "colab_type": "text"
      },
      "source": [
        "We download the Spacy lemmatizer again to reduce words to their lemma for normalization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4WT6aPZTtKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install es-lemmatizer\n",
        "!pip install -U spacy\n",
        "!sudo python -m spacy download es_core_news_sm\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PAEUqxOUGuQ",
        "colab_type": "text"
      },
      "source": [
        "We load in the universe of articles collected from EMIS using the scripts in EMIS_scrape repository. \n",
        "\n",
        "These articles are downloaded from subnational news sources, regional newspapers, and other sources not specified as national newspapers. There's a lot of noise in these articles. I leave the training articles in the universe since the model should perform well on the articles it was trained on. \n",
        "\n",
        "This csv contains 158,496 articles. The majority of these are noise!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zObSCC7hoXm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "d7c3a6a3-6546-4dfa-b201-c2c2778b08e2"
      },
      "source": [
        " emis = pd.read_csv('My Drive/Data/OCVED/Classifier/universe/EMIS_Universe.csv')\n",
        " emis"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>date</th>\n",
              "      <th>file_id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2014 09 04</td>\n",
              "      <td>20140904__455079294.txt</td>\n",
              "      <td>\\nQué leer\\n\\nKaren López\\n20140904.-AMOR Y OT...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2014 08 13</td>\n",
              "      <td>20140813__452145796.txt</td>\n",
              "      <td>\\nRoban carteras\\n\\n\\n20140813.-Aprovechando l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2014 07 18</td>\n",
              "      <td>20140718__449892055.txt</td>\n",
              "      <td>\\n                                        Poli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2014 07 21</td>\n",
              "      <td>20140721__450060451.txt</td>\n",
              "      <td>\\nMás cargos encima\\n\\nMURAL / STAFF\\n20140721...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2014 07 17</td>\n",
              "      <td>20140717__449726199.txt</td>\n",
              "      <td>\\nExcélsior | 2014-07-17 | 10:00\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158492</th>\n",
              "      <td>158492</td>\n",
              "      <td>2015 10 12</td>\n",
              "      <td>20151012__504376451.txt</td>\n",
              "      <td>\\n El maquillaje, si se elige adecuadamente, p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158493</th>\n",
              "      <td>158493</td>\n",
              "      <td>2015 02 08</td>\n",
              "      <td>20150208__469314532.txt</td>\n",
              "      <td>\\n                                        POR ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158494</th>\n",
              "      <td>158494</td>\n",
              "      <td>2015 09 28</td>\n",
              "      <td>20150928__502939710.txt</td>\n",
              "      <td>\\n Aún no se saben los motivos que llevaron a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158495</th>\n",
              "      <td>158495</td>\n",
              "      <td>2015 03 12</td>\n",
              "      <td>20150312__473098625.txt</td>\n",
              "      <td>\\nMatan a precandidata del PRD en Guerrero\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158496</th>\n",
              "      <td>158496</td>\n",
              "      <td>2015 06 05</td>\n",
              "      <td>20150605__484463377.txt</td>\n",
              "      <td>\\nDe la Redacción  | 2015-06-04 | 21:05\\n\\n\\n\\...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>158497 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0  ...                                               text\n",
              "0                0  ...  \\nQué leer\\n\\nKaren López\\n20140904.-AMOR Y OT...\n",
              "1                1  ...  \\nRoban carteras\\n\\n\\n20140813.-Aprovechando l...\n",
              "2                2  ...  \\n                                        Poli...\n",
              "3                3  ...  \\nMás cargos encima\\n\\nMURAL / STAFF\\n20140721...\n",
              "4                4  ...  \\nExcélsior | 2014-07-17 | 10:00\\n\\n\\n\\n\\n\\n\\n...\n",
              "...            ...  ...                                                ...\n",
              "158492      158492  ...  \\n El maquillaje, si se elige adecuadamente, p...\n",
              "158493      158493  ...  \\n                                        POR ...\n",
              "158494      158494  ...  \\n Aún no se saben los motivos que llevaron a ...\n",
              "158495      158495  ...  \\nMatan a precandidata del PRD en Guerrero\\n\\n...\n",
              "158496      158496  ...  \\nDe la Redacción  | 2015-06-04 | 21:05\\n\\n\\n\\...\n",
              "\n",
              "[158497 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khscjaZcUpv2",
        "colab_type": "text"
      },
      "source": [
        "As detailed in script 1, a seperate process collected articles from national newspapers by having RA's manually download these articles. The manual download process took 5 months, students would read each article and determine if it was relevant to the PI's research. In contrast, the scraping and generating traning data took a total of 3 months, with the added advantage that the model can be used for future data collected. \n",
        "\n",
        "This process generated 29,995 articles. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cti-eVZsVRDM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "1d5678d2-e4a9-49b8-a05a-72a8fee6407a"
      },
      "source": [
        "nat = pd.read_csv('My Drive/Data/OCVED/National/txt_docs/National_OCVED.csv')\n",
        "nat"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>file_id</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5 de Enero de 2000 \\nTranslation powered by Go...</td>\n",
              "      <td>20000105001_NAC.txt</td>\n",
              "      <td>Accept</td>\n",
              "      <td>\\n es \\n \\n \\n Las Margaritas, Chis., 5 Ene (N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4 de Enero de 2000 \\nTranslation powered by Go...</td>\n",
              "      <td>20000105002_NAC.txt</td>\n",
              "      <td>Accept</td>\n",
              "      <td>\\n es \\n \\n \\n México, 4 Ene (NTX).- La Policí...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6 de Enero de 2000 \\nTranslation powered by Go...</td>\n",
              "      <td>20000106001_NAC.txt</td>\n",
              "      <td>Accept</td>\n",
              "      <td>\\n es \\n \\n \\n México, 6 Ene (NTX).- Elementos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6 de Enero de 2000 \\nTranslation powered by Go...</td>\n",
              "      <td>20000106002_NAC.txt</td>\n",
              "      <td>Accept</td>\n",
              "      <td>\\n es \\n \\n \\n Monterrey, NL., 6 Ene (NTX).- L...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6 de Enero de 2000 \\nTranslation powered by Go...</td>\n",
              "      <td>20000106003_NAC.txt</td>\n",
              "      <td>Accept</td>\n",
              "      <td>\\n es \\n \\n \\n México, 6 Ene (NTX).- Elementos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29990</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2018112401_NAT.txt</td>\n",
              "      <td>Accept</td>\n",
              "      <td>\\n \\n \\n \\n \\n \\n November 24, 2018 \\n | \\n Pu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29991</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2018112402_NAT.txt</td>\n",
              "      <td>Accept</td>\n",
              "      <td>\\n \\n \\n \\n \\n \\n November 24, 2018 (15:07) \\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29992</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2018112501_NAT.txt</td>\n",
              "      <td>Accept</td>\n",
              "      <td>\\n \\n \\n \\n \\n \\n November 25, 2018 \\n | \\n Pu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29993</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2018112801_NAT.txt</td>\n",
              "      <td>Accept</td>\n",
              "      <td>\\n \\n \\n \\n \\n \\n November 28, 2018 \\n | \\n Pu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29994</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2018112802_NAT.txt</td>\n",
              "      <td>Accept</td>\n",
              "      <td>\\n \\n \\n \\n \\n \\n November 28, 2018 (12:34) \\n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>29995 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    date  ...                                               text\n",
              "0      5 de Enero de 2000 \\nTranslation powered by Go...  ...  \\n es \\n \\n \\n Las Margaritas, Chis., 5 Ene (N...\n",
              "1      4 de Enero de 2000 \\nTranslation powered by Go...  ...  \\n es \\n \\n \\n México, 4 Ene (NTX).- La Policí...\n",
              "2      6 de Enero de 2000 \\nTranslation powered by Go...  ...  \\n es \\n \\n \\n México, 6 Ene (NTX).- Elementos...\n",
              "3      6 de Enero de 2000 \\nTranslation powered by Go...  ...  \\n es \\n \\n \\n Monterrey, NL., 6 Ene (NTX).- L...\n",
              "4      6 de Enero de 2000 \\nTranslation powered by Go...  ...  \\n es \\n \\n \\n México, 6 Ene (NTX).- Elementos...\n",
              "...                                                  ...  ...                                                ...\n",
              "29990                                                NaN  ...  \\n \\n \\n \\n \\n \\n November 24, 2018 \\n | \\n Pu...\n",
              "29991                                                NaN  ...  \\n \\n \\n \\n \\n \\n November 24, 2018 (15:07) \\n...\n",
              "29992                                                NaN  ...  \\n \\n \\n \\n \\n \\n November 25, 2018 \\n | \\n Pu...\n",
              "29993                                                NaN  ...  \\n \\n \\n \\n \\n \\n November 28, 2018 \\n | \\n Pu...\n",
              "29994                                                NaN  ...  \\n \\n \\n \\n \\n \\n November 28, 2018 (12:34) \\n...\n",
              "\n",
              "[29995 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKSFaMfhVJJP",
        "colab_type": "text"
      },
      "source": [
        "We combine these two datasets, making a full universe of articles on DTO's in Mexico. All articles used in the training steps are also included given the model should perform well classifying these. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WELYamrVV3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = []\n",
        "data.append(emis)\n",
        "data.append(nat)\n",
        "df = pd.concat(data, axis=0, ignore_index=True, sort=True).sort_values('file_id', ascending= True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8TCmdZTiqoT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "16077288-8ebf-44f3-91d3-32b31fe617c0"
      },
      "source": [
        "df"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>date</th>\n",
              "      <th>file_id</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>158497</th>\n",
              "      <td>NaN</td>\n",
              "      <td>5 de Enero de 2000 \\nTranslation powered by Go...</td>\n",
              "      <td>20000105001_NAC.txt</td>\n",
              "      <td>Accept</td>\n",
              "      <td>\\n es \\n \\n \\n Las Margaritas, Chis., 5 Ene (N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158498</th>\n",
              "      <td>NaN</td>\n",
              "      <td>4 de Enero de 2000 \\nTranslation powered by Go...</td>\n",
              "      <td>20000105002_NAC.txt</td>\n",
              "      <td>Accept</td>\n",
              "      <td>\\n es \\n \\n \\n México, 4 Ene (NTX).- La Policí...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158499</th>\n",
              "      <td>NaN</td>\n",
              "      <td>6 de Enero de 2000 \\nTranslation powered by Go...</td>\n",
              "      <td>20000106001_NAC.txt</td>\n",
              "      <td>Accept</td>\n",
              "      <td>\\n es \\n \\n \\n México, 6 Ene (NTX).- Elementos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158500</th>\n",
              "      <td>NaN</td>\n",
              "      <td>6 de Enero de 2000 \\nTranslation powered by Go...</td>\n",
              "      <td>20000106002_NAC.txt</td>\n",
              "      <td>Accept</td>\n",
              "      <td>\\n es \\n \\n \\n Monterrey, NL., 6 Ene (NTX).- L...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158501</th>\n",
              "      <td>NaN</td>\n",
              "      <td>6 de Enero de 2000 \\nTranslation powered by Go...</td>\n",
              "      <td>20000106003_NAC.txt</td>\n",
              "      <td>Accept</td>\n",
              "      <td>\\n es \\n \\n \\n México, 6 Ene (NTX).- Elementos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28601</th>\n",
              "      <td>28601.0</td>\n",
              "      <td>2018 12 31</td>\n",
              "      <td>20181231__639443971.txt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\\nLos hechos ocurrieron esta mañana en la colo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27641</th>\n",
              "      <td>27641.0</td>\n",
              "      <td>2018 12 31</td>\n",
              "      <td>20181231__639443972.txt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\\nHay dos personas lesionadas. am.  Un ataque ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39623</th>\n",
              "      <td>39623.0</td>\n",
              "      <td>2018 12 31</td>\n",
              "      <td>20181231__639453461.txt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\\n  El horario para dejar los juguetes es de 9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30079</th>\n",
              "      <td>30079.0</td>\n",
              "      <td>2018 12 31</td>\n",
              "      <td>20181231__639501374.txt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\\n                                        Un h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34960</th>\n",
              "      <td>34960.0</td>\n",
              "      <td>2018 12 31</td>\n",
              "      <td>20181231__640897761.txt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\\n\\nHidalgo del Parral.- La Fiscalía General d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>188492 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0  ...                                               text\n",
              "158497         NaN  ...  \\n es \\n \\n \\n Las Margaritas, Chis., 5 Ene (N...\n",
              "158498         NaN  ...  \\n es \\n \\n \\n México, 4 Ene (NTX).- La Policí...\n",
              "158499         NaN  ...  \\n es \\n \\n \\n México, 6 Ene (NTX).- Elementos...\n",
              "158500         NaN  ...  \\n es \\n \\n \\n Monterrey, NL., 6 Ene (NTX).- L...\n",
              "158501         NaN  ...  \\n es \\n \\n \\n México, 6 Ene (NTX).- Elementos...\n",
              "...            ...  ...                                                ...\n",
              "28601      28601.0  ...  \\nLos hechos ocurrieron esta mañana en la colo...\n",
              "27641      27641.0  ...  \\nHay dos personas lesionadas. am.  Un ataque ...\n",
              "39623      39623.0  ...  \\n  El horario para dejar los juguetes es de 9...\n",
              "30079      30079.0  ...  \\n                                        Un h...\n",
              "34960      34960.0  ...  \\n\\nHidalgo del Parral.- La Fiscalía General d...\n",
              "\n",
              "[188492 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y174VRg3Tu1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1000)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bya_4aVVSD-",
        "colab_type": "text"
      },
      "source": [
        "Code for removing accents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQUNfsMbTw96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unicodedata\n",
        "import string\n",
        "# BEGIN SHAVE_MARKS_LATIN\n",
        "def shave_marks_latin(txt):\n",
        "    \"\"\"Remove all diacritic marks from Latin base characters\"\"\"\n",
        "    norm_txt = unicodedata.normalize('NFD', txt)  # <1>\n",
        "    latin_base = False\n",
        "    keepers = []\n",
        "    for c in norm_txt:\n",
        "        if unicodedata.combining(c) and latin_base:   # <2>\n",
        "            continue  # ignore diacritic on Latin base char\n",
        "        keepers.append(c)                             # <3>\n",
        "        # if it isn't combining char, it's a new base char\n",
        "        if not unicodedata.combining(c):              # <4>\n",
        "            latin_base = c in string.ascii_letters\n",
        "    shaved = ''.join(keepers)\n",
        "    return unicodedata.normalize('NFC', shaved)   # <5>\n",
        "# END SHAVE_MARKS_LATIN\n",
        "def shave_marks(txt):\n",
        "    \"\"\"Remove all diacritic marks\"\"\"\n",
        "    norm_txt = unicodedata.normalize('NFD', txt)  # <1>\n",
        "    shaved = ''.join(c for c in norm_txt\n",
        "                     if not unicodedata.combining(c))  # <2>\n",
        "    return unicodedata.normalize('NFC', shaved)  # <3>\n",
        "# END SHAVE_MARKS"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yABnHpnwVUkd",
        "colab_type": "text"
      },
      "source": [
        "Let's load the tokenizer and lemmatizer in. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96FmM3zZTxt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from es_lemmatizer import lemmatize\n",
        "import es_core_news_sm\n",
        "\n",
        "nlp = es_core_news_sm.load()\n",
        "nlp.add_pipe(lemmatize, after=\"tagger\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2wzqVLIVXgk",
        "colab_type": "text"
      },
      "source": [
        "Stopwords removed to reduce noise and reduce the number of not useful features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf1DOC5STzqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "##Creating a list of stop words and adding custom stopwords\n",
        "stop_words = set(stopwords.words(\"spanish\"))\n",
        "##Creating a list of custom stopwords\n",
        "new_words = [\"daily\", \"newspaper\", \"reforma\", \"publication\", \"universal\", \"iv\", \"one\", \"two\", \"august\" , \"excelsior\", \"online\",\n",
        "             \"november\", \"july\", \"september\", \"june\", \"october\", \"december\", \"print\", \"edition\", \"news\", \"milenio\", \"january\", \"international\",\n",
        "             \"march\", \"april\", \"july\", \"february\", \"may\", \"october\", \"el occidental\", \"comments\", \"powered\", \"display\", \"space\", \n",
        "             \"javascript\", \"trackpageview\", \"enablelinktracking\", \"location\", \"protocol\", \"weboperations\", \"settrackerurl\", \"left\", \n",
        "             \"setsiteid\", \"createelement\", \"getelementsbytagname\", \"parentnode\", \"insertbefore\", \"writeposttexto\", \"everykey\", \"passwords\"\n",
        "             \"writecolumnaderechanotas\", \"anteriorsiguente\", \"anteriorsiguiente\", \"writefooter\", \"align\", \"googletag\", \"writeaddthis\", \"writefooteroem\", \n",
        "             \"diario delicias\", \"diario tampico\", \"the associated press\", \"redaccion\" , \"national\", \"diario yucatan\", \"mural\", \"periodico\", \n",
        "             \"new\", \"previously\", \"shown\" , \"a\", \"para\", \"tener\" , \"haber\", \"ser\" , \"mexico city\", \"states\", \"city\", \"and\", \"elsolde\", \"recomendamos\", \n",
        "            \"diario chihuahua\" , \"diario juarez\" , \"el norte\", \"voz frontera\" , \"regional\" , \"de\"  , \"el sol\" , \"el\" , \"sudcaliforniano\" , \"washington\",\n",
        "            \"union morelos\", \"milenio\" , \"notimex\", \"el financiero\" , \"financiero\" , \"forum magazine\" , \"economista\" , \"gmail\" , \"financial\", \"el\" , \"de\",\n",
        "             \"la\", \"del\", \"de+el\" , \"a+el\" , \"shortcode\" , \"caption\", \"cfcfcf\", \"float\", \"item\", \"width\", \"follow\", \"aaannnnn\", \"gmannnnn\", \n",
        "             \"dslnnnnn\", \"jtjnnnnn\", \"lcgnnnnn\", \"jgcnnnnn\", \"vhannnnn\",  \"mtc\", \"eleconomista\", \"monitoreoif\", \"infosel\", \"gallery\", \n",
        "             \"heaven\", \"div\", \"push\" , \"translate\", \"google\"]\n",
        "stop_words = stop_words.union(new_words)\n",
        "stop_words = shave_marks(repr(stop_words))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZhHgU2UT211",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = df"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRBN1KKEVeKr",
        "colab_type": "text"
      },
      "source": [
        "Process for cleaning out the text and generating the corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k8OGP1CT6mU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e3ebfe17-d509-4005-8229-4538926d2095"
      },
      "source": [
        "corpus = []\n",
        "for i in dataset.itertuples():\n",
        "#for i in tqdm(range(1, 2000)):\n",
        "    text = shave_marks_latin(i.text)\n",
        "    #Remove punctuations\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    #Convert to lowercase\n",
        "    #text = shave_marks_latin(text)\n",
        "    #text = text.lower()\n",
        "    #remove tags\n",
        "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "    # remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    #Lemmatisation\n",
        "    doc = nlp(text)\n",
        "    text = [token.lemma_ for token in doc if token.lemma_ not in stop_words] \n",
        "    text = \" \".join(text)\n",
        "    text = shave_marks(text)\n",
        "    file_id = i.file_id\n",
        "    original = i.text\n",
        "    corpus.append({ 'text': text, 'file_id': file_id , \"original\": original})\n",
        "print (\"done\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuku5rS0VDsm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "0fbc7f81-8418-4f4b-e4da-22c67dd7a2aa"
      },
      "source": [
        "data = pd.DataFrame(corpus)\n",
        "data"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>file_id</th>\n",
              "      <th>original</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>margaritas chis ntx elementos ejercito exicano...</td>\n",
              "      <td>20000105001_NAC.txt</td>\n",
              "      <td>\\n es \\n \\n \\n Las Margaritas, Chis., 5 Ene (N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ntx policia federal preventiva pfp informo ult...</td>\n",
              "      <td>20000105002_NAC.txt</td>\n",
              "      <td>\\n es \\n \\n \\n México, 4 Ene (NTX).- La Policí...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ntx elementos policia judicial federal pjf ase...</td>\n",
              "      <td>20000106001_NAC.txt</td>\n",
              "      <td>\\n es \\n \\n \\n México, 6 Ene (NTX).- Elementos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>monterrey ntx policia ministerial reporto homb...</td>\n",
              "      <td>20000106002_NAC.txt</td>\n",
              "      <td>\\n es \\n \\n \\n Monterrey, NL., 6 Ene (NTX).- L...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ntx elementos policia judicial federal pjf ase...</td>\n",
              "      <td>20000106003_NAC.txt</td>\n",
              "      <td>\\n es \\n \\n \\n México, 6 Ene (NTX).- Elementos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188487</th>\n",
              "      <td>hecho ocurrir manana colonia leon guzman salam...</td>\n",
              "      <td>20181231__639443971.txt</td>\n",
              "      <td>\\nLos hechos ocurrieron esta mañana en la colo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188488</th>\n",
              "      <td>persona lesionadas ataque domicilio colonia ba...</td>\n",
              "      <td>20181231__639443972.txt</td>\n",
              "      <td>\\nHay dos personas lesionadas. am.  Un ataque ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188489</th>\n",
              "      <td>horario dejar juguete manana tarde instalacion...</td>\n",
              "      <td>20181231__639453461.txt</td>\n",
              "      <td>\\n  El horario para dejar los juguetes es de 9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188490</th>\n",
              "      <td>hombre asesinar casa villagran hombre perdio v...</td>\n",
              "      <td>20181231__639501374.txt</td>\n",
              "      <td>\\n                                        Un h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188491</th>\n",
              "      <td>hidalgo parral fiscalia general fge investigar...</td>\n",
              "      <td>20181231__640897761.txt</td>\n",
              "      <td>\\n\\nHidalgo del Parral.- La Fiscalía General d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>188492 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  ...                                           original\n",
              "0       margaritas chis ntx elementos ejercito exicano...  ...  \\n es \\n \\n \\n Las Margaritas, Chis., 5 Ene (N...\n",
              "1       ntx policia federal preventiva pfp informo ult...  ...  \\n es \\n \\n \\n México, 4 Ene (NTX).- La Policí...\n",
              "2       ntx elementos policia judicial federal pjf ase...  ...  \\n es \\n \\n \\n México, 6 Ene (NTX).- Elementos...\n",
              "3       monterrey ntx policia ministerial reporto homb...  ...  \\n es \\n \\n \\n Monterrey, NL., 6 Ene (NTX).- L...\n",
              "4       ntx elementos policia judicial federal pjf ase...  ...  \\n es \\n \\n \\n México, 6 Ene (NTX).- Elementos...\n",
              "...                                                   ...  ...                                                ...\n",
              "188487  hecho ocurrir manana colonia leon guzman salam...  ...  \\nLos hechos ocurrieron esta mañana en la colo...\n",
              "188488  persona lesionadas ataque domicilio colonia ba...  ...  \\nHay dos personas lesionadas. am.  Un ataque ...\n",
              "188489  horario dejar juguete manana tarde instalacion...  ...  \\n  El horario para dejar los juguetes es de 9...\n",
              "188490  hombre asesinar casa villagran hombre perdio v...  ...  \\n                                        Un h...\n",
              "188491  hidalgo parral fiscalia general fge investigar...  ...  \\n\\nHidalgo del Parral.- La Fiscalía General d...\n",
              "\n",
              "[188492 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfFQQ6NgX-oi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen = data['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5UY7DISYI0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_PKvQnWVsX9",
        "colab_type": "text"
      },
      "source": [
        "Let's look at how frequent some words are in the universe using Tokenizer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFPc42V_VmiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "#Using TensorFlow backend. xtrain_count, train_y, xvalid_count\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(gen)\n",
        "\n",
        "X_gen = tokenizer.texts_to_sequences(gen)\n",
        "#X_test = tokenizer.texts_to_sequences(valid_x)\n",
        "#\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "\n",
        "print(gen.iloc[3])\n",
        "print(X_gen[3])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RvlFJsrYj4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word in ['sexual', 'cartel', 'sinaloa', 'violencia']:\n",
        "  print('{}: {}'.format(word, tokenizer.word_index[word]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkL8fZhXYpHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 100\n",
        "\n",
        "X_gen = pad_sequences(X_gen, padding='post') #, maxlen=maxlen\n",
        "#X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "print(X_gen[2, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaLrx0SjVwxC",
        "colab_type": "text"
      },
      "source": [
        "# Application\n",
        "\n",
        "Now we load in the encoder, model, and vectorizer from script 1 so we can implement it in the application pipeline. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LCFVkX1ZMwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "pkl_file = open('/content/drive/My Drive/Data/OCVED/Classifier/algorithm/OCVED_encoder_v2.pkl', 'rb')\n",
        "encoder = pickle.load(pkl_file) \n",
        "pkl_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TuXtLw_V5ut",
        "colab_type": "text"
      },
      "source": [
        "We used the LR model because it produced the best F1 score of all models. See Osorio & Beltran (2020) for more information on why. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWTrwHpLgHAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.externals import joblib\n",
        "# save the model to disk\n",
        "filename = '/content/drive/My Drive/Data/OCVED/Classifier/algorithm/logistic_model_v2.sav'\n",
        "  \n",
        "# load the model from disk\n",
        "logit_model = joblib.load(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBd1XGpgWDue",
        "colab_type": "text"
      },
      "source": [
        "It's important we use the same trained tfidf from the first script in this process. Otherwise the length and words used will be different across vectors!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAJSFHbriay9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a count vectorizer object \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import pickle\n",
        "\n",
        "#count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "#count_vect.fit(data['text'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "#xtrain_count =  count_vect.transform(gen)\n",
        "#xtrain_count\n",
        "\n",
        "#pickle.dump(xtrain_count, open(\"/content/drive/My Drive/Data/Bogota/categorized_articles/tfidf.pickle\", \"wb\"))\n",
        "\n",
        "pkl_file = open(\"/content/drive/My Drive/Data/OCVED/Classifier/algorithm/Tfidf_vect_3.pickle\", 'rb')\n",
        "tfidf = pickle.load(pkl_file) \n",
        "pkl_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQLNYLRromJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ7E1udpmzxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_2 = tfidf.transform(gen)\n",
        "gen_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZIeewWXWLFB",
        "colab_type": "text"
      },
      "source": [
        "Now we finally ask the logit model to generate predictions for each article. It reviews the numeric contents and makes a predictions. Anything that has above a .5 probability of being DTO related is classified as such. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8smaa2AgLeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make a prediction\n",
        "y_label = logit_model.predict(gen_2)\n",
        "# show the inputs and predicted outputs\n",
        "print(\"X=%s, Predicted=%s\" % (gen_2[0], y_label[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V2d9IySgRA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make a prediction\n",
        "y_prob = logit_model.predict_proba(gen_2)[:,1]\n",
        "# show the inputs and predicted outputs\n",
        "y_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPDd2pUGWU-M",
        "colab_type": "text"
      },
      "source": [
        "Now we want to save the output, first in a csv. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvL0c-GKgXxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['y_label'] = y_label\n",
        "\n",
        "data['y_prob'] = y_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNYIpIO5gbrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.to_csv('My Drive/Data/OCVED/Classifier/predictions_v3/logit_OCVED_pred_v3.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUfOYeBUwMsg",
        "colab_type": "text"
      },
      "source": [
        "Here I save them as .txt files for use in Eventus ID. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg1RiyWp4UxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data[data.y_label == 1 ]\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4CMJXJhyv6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in tqdm(dataset.itertuples()):\n",
        "    text = shave_marks_latin(i.text)\n",
        "    #Remove punctuations\n",
        "    #text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    #Convert to lowercase\n",
        "    #remove tags\n",
        "    #text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "    # remove special characters and digits\n",
        "    #text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "    #text = re.sub(' +', ' ', text)\n",
        "    file_id = i.file_id\n",
        "    original = i.original\n",
        "    dirty = 'My Drive/Data/OCVED/Classifier/predictions_v3/dirty/'\n",
        "    clean = 'My Drive/Data/OCVED/Classifier/predictions_v3/clean/'\n",
        "    dirty_file = dirty + file_id\n",
        "    clean_file = clean + file_id\n",
        "    with open(dirty_file, 'w') as f:\n",
        "      f.write(original)\n",
        "    with open(clean_file, 'w') as c:\n",
        "      c.write(text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0r2ut9P_oV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"script has completed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePX7D7sZWdTA",
        "colab_type": "text"
      },
      "source": [
        "This takes a long time so I have it print out the time it finished. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWAIdyZxHU69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/America/Phoenix /etc/localtime\n",
        "!date"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}